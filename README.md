## **Wikipedia Recommender System**

Welcome to our project repository for the Network Tour of Data Science course at EPFL !

We implemented a query-based search engine for Wikipedia articles related to various Machine Learning topics. 

In other words, given a query our system will retrieve and suggest articles with similar semantic contents. Moreover, we provide a graph visualisation tool to interact with the query engine.

More details about this ML system can be found in the project [report](report.pdf).

### How to reproduce results:

- Specify INITIAL_FILENAME in [config.py](config.py). This is the path to the dataset produced on [Seealsology](https://densitydesign.github.io/strumentalia-seealsology/) (to put in the [data](/data) folder). The seeds to scrap the graph are given in the [seeds_seealsology.txt](/data/seeds_seealsology.txt)  file (use a max distance of 2).
- Download the [wiki-news-300d-1M-subword.magnitude](http://magnitude.plasticity.ai/fasttext/light/wiki-news-300d-1M-subword.magnitude) file at and put it into the [data](/data) folder.
- `export PYTHONPATH='wd'` where wd is the directory containing the [run.sh](run.sh) script.
- Execute the [run.sh](run.sh) script.
- Run [exploration.ipynb](exploration.ipynb) and/or [exploitation.ipynb](exploitation/exploitation.ipynb) for the respective analysis.

### Interactive Visualisation:
After having done the previous part, run the command: `sudo PYTHONPATH='wd' python3 visualization/app.py`

You can choose any of the three methods to perform a query. 

*For multiple concepts, please separate by a comma, e.g. machine learning,text processing*
The port 80 must be opened for external access if you use a server.

- By clicking on a node, 'Chosen node' link will redirect you to the corresponding web page.
- Only the page title of nodes that best fit the query as well as the neighbours are shown.
- Red edges mean that the pages are present in the 'See also' section on Wikipedia website.
- The color of the nodes represents the cosine similarity score.

### Files breakdown:

[run.sh](run.sh) : shell script executing the acquisition, exploitation and visualisation tasks.


Acquisition:
- [acquisition_helpers.py](acquisition/acquisition_helpers.py) :  various helpers for the acquisition.py script,
- [acquisition.py](acquisition/acquisition.py) : loads the dataset and augments it with urls and keywords extraction. Create df_node dataframe which contains node information and df_edge which contains edge relation.

Exploration:
- [exploration.ipynb](exploration.ipynb): exploratory data analysis.

Exploitation:
- [exploitation.py](exploitation/exploitation.py): fits and saves the 3 models we used
- [exploitation.ipynb](exploitation/exploitation.ipynb): loads the models and performs a qualitative evaluation on a set of queries and topics

Visualization:
- [app.py](visualization/app.py): runs the visualisation app on a dedicated server
- [create_visu.py](visualization/create_visu.py): creates and saves the graph visualisation
- [utils.py](visualization/utils.py): various helpers

Helpers:
- [predict.py](helpers/predict.py): helpers for the exploitation part
- [spectral_clustering.py](helpers/spectral_clustering.py): specific helpers for the spectral clustering model

Data:
- [Data](/data): contains every file loaded and generated by the different modules.

### Authors
- EL Amrani Ayyoub
- Micheli Vincent
- Myotte Frédéric
- Sinnathamby Karthigan

*Network Tour of Data Science course - EPFL - Fall 2019 - Team 2*